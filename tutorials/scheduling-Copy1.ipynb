{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "from boto3 import resource\n",
    "import sys\n",
    "\n",
    "from grizly.scheduling.registry import Job\n",
    "from grizly import Email, S3, config\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"GRIZLY_REDIS_HOST\"] = \"10.125.68.177\"\n",
    "os.environ[\"GRIZLY_REDIS_HOST\"] = \"pytest_redis\"\n",
    "os.environ[\"GRIZLY_DASK_SCHEDULER_ADDRESS\"] = \"10.125.68.177:8999\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you register a job you have to define tasks that your job will run. Let's define a function that returns last modified date of a file in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "@dask.delayed\n",
    "def get_last_modified_date(full_s3_key, logger):\n",
    "    bucket = \"acoe-s3\"\n",
    "    date = resource(\"s3\").Object(bucket, full_s3_key).last_modified\n",
    "#     import time\n",
    "    time.sleep(2)\n",
    "    logger = logging.getLogger(\"dask.distributed\")\n",
    "    logger.warning(\"test in dask\")\n",
    "    print(\"hej2\")\n",
    "    raise ValueError(\"ERROR\")\n",
    "    return str(date)\n",
    "\n",
    "def main():\n",
    "    logger = logging.getLogger(\"grizly\").getChild(\"custom\")\n",
    "    logger.info(\"test in main\")\n",
    "    print(\"hej3\")\n",
    "    logger.info(\"test in main 2\")\n",
    "    return get_last_modified_date(full_s3_key=\"grizly/test_scheduling.csv\", logger=logger).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_last_modified_date(full_s3_key=\"grizly/test_scheduling.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs that are listening for some changes are called **listener jobs**. A good practice is to end their name with `listener` suffix so that they are easy to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-10 16:17:41,160 - k - INFO - Job s3_grizly_test_scheduling_listener successfully removed from registry\n",
      "2020-12-10 16:17:41,173 - k - INFO - Job s3_grizly_test_scheduling_listener successfully registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Job(name='s3_grizly_test_scheduling_listener')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = Job(\"s3_grizly_test_scheduling_listener\", logger=logging.getLogger(\"k\"))\n",
    "\n",
    "job.register(main, \n",
    "#              \"grizly/test_scheduling.csv\",\n",
    "             if_exists=\"replace\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just registered a job called `s3_grizly_test_scheduling_listener`. The name of the job is unique and you can always check its details with `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: s3_grizly_test_scheduling_listener\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-12-10 15:26:47.176786+00:00\n",
      "crons: []\n",
      "downstream: {}\n",
      "upstream: {}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "job = Job(\"s3_grizly_test_scheduling_listener\")\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this job is not scheduled yet - it's not a cron job and it doesn't have any upstream jobs and it doesn't have any triggers. You can pass these parameters during registration or overwrite them later using `crons`, `upstream` or `triggers` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cron string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add now a cron string to our job to run every two hours. You can generate cron string using this website https://crontab.guru/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: s3_grizly_test_scheduling_listener\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-12-10 14:05:16.521340+00:00\n",
      "crons: ['0 */2 * * *']\n",
      "downstream: {}\n",
      "upstream: {}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "job.crons = \"0 */2 * * *\"\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your job imediately using `submit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-10 16:28:45,003 - k - INFO - Submitting job s3_grizly_test_scheduling_listener...\n",
      "2020-12-10 16:28:45,011 - grizly.custom - INFO - test in main\n",
      "2020-12-10 16:28:45,011 - grizly.custom - INFO - test in main 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hej3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-10 16:28:47,466 - k - INFO - Job s3_grizly_test_scheduling_listener finished with status fail\n"
     ]
    }
   ],
   "source": [
    "job.submit(scheduler_address=\"10.125.68.177:8999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "name: None\n",
      "created_at: 2020-12-10 16:17:49.212585+00:00\n",
      "finished_at: 2020-12-10 16:17:51.736016+00:00\n",
      "duration: 2\n",
      "status: fail\n",
      "error: ERROR\n",
      "result: None\n"
     ]
    }
   ],
   "source": [
    "job.last_run.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<StreamHandler (NOTSET)>, <StreamHandler (NOTSET)>, <StreamHandler (NOTSET)>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.getLogger(\"grizly\").handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check job's last run details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first run you will be able to access `last_run` property with information about the last run of your job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test in main\n",
      "test in main 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(job.last_run.logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now update the file and run the job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:20,698 | INFO : Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2020-12-01 08:35:22,167 | INFO : Successfully uploaded 'test_scheduling.csv' to S3\n"
     ]
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = DataFrame(data=d)\n",
    "\n",
    "s3 = S3(s3_key=\"grizly/\", file_name=\"test_scheduling.csv\").from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:23,838 | INFO : Submitting job s3_grizly_test_scheduling_listener...\n",
      "2020-12-01 08:35:26,598 | INFO : Job s3_grizly_test_scheduling_listener finished with status success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-12-01 08:35:22+00:00']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit(scheduler_address=\"10.125.68.177:8999\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 2\n",
      "name: None\n",
      "created_at: 2020-12-01 08:35:24.074831+00:00\n",
      "finished_at: 2020-12-01 08:35:26.600356+00:00\n",
      "duration: 1\n",
      "status: success\n",
      "error: None\n",
      "result: ['2020-12-01 08:35:22+00:00']\n"
     ]
    }
   ],
   "source": [
    "job.last_run.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register jobs with upstream job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now register two jobs with upstream job `s3_grizly_test_scheduling_listener`. One will send an email whenever upstream finished with status `success` and the other will send an email whenever the upstream changed his result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def send_email(subject, body, to):\n",
    "    logger = logging.getLogger(\"distributed.worker\").getChild(\"email\")\n",
    "    e = Email(subject=subject, body=body, logger=logger)\n",
    "    e.send(to=to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:32,926 | INFO : Job email_upstream_success successfully registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: email_upstream_success\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-12-01 08:35:30.801676+00:00\n",
      "crons: []\n",
      "downstream: {}\n",
      "upstream: {'s3_grizly_test_scheduling_listener': 'success'}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "to = config.get_service(\"email\").get(\"address\")\n",
    "\n",
    "task = send_email(subject=\"Job success\",\n",
    "                   body=\"Job `s3_grizly_test_scheduling_listener` finished with status success.\", \n",
    "                   to=to)\n",
    "\n",
    "job = Job(\"email_upstream_success\")\n",
    "\n",
    "job.register(tasks=[task], \n",
    "             if_exists=\"replace\",\n",
    "             upstream={\"s3_grizly_test_scheduling_listener\": \"success\"}\n",
    "             )\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:37,652 | INFO : Job email_upstream_result_change successfully registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: email_upstream_result_change\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-12-01 08:35:35.535265+00:00\n",
      "crons: []\n",
      "downstream: {}\n",
      "upstream: {'s3_grizly_test_scheduling_listener': 'result_change'}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "to = config.get_service(\"email\").get(\"address\")\n",
    "\n",
    "task = send_email(subject=\"File changed\",\n",
    "                   body=\"Somebody changed 'grizly/test_scheduling.csv' file!\", \n",
    "                   to=to)\n",
    "\n",
    "job = Job(\"email_upstream_result_change\")\n",
    "\n",
    "job.register(tasks=[task], \n",
    "               if_exists=\"replace\",\n",
    "               upstream={\"s3_grizly_test_scheduling_listener\": \"result_change\"}\n",
    "              )\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now that `s3_grizly_test_scheduling_listener` has two downstream jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: s3_grizly_test_scheduling_listener\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-12-01 08:35:06.592377+00:00\n",
      "crons: ['0 */2 * * *']\n",
      "downstream: {'email_upstream_success': 'success', 'email_upstream_result_change': 'result_change'}\n",
      "upstream: {}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "job = Job(\"s3_grizly_test_scheduling_listener\")\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now submit the listener job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:43,590 | INFO : Submitting job s3_grizly_test_scheduling_listener...\n",
      "2020-12-01 08:35:46,352 | INFO : Job s3_grizly_test_scheduling_listener finished with status success\n",
      "2020-12-01 08:35:50,294 | INFO : Job email_upstream_success has been enqueued\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-12-01 08:35:22+00:00']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit(scheduler_address=\"10.125.68.177:8999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `s3_grizly_test_scheduling_listener` job finished with status success and enqueued his downstream job `email_upstream_succcess`. Let's now change the file in s3 and run our listener job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:51,819 | INFO : Successfully uploaded 'test_scheduling.csv' to S3\n"
     ]
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = DataFrame(data=d)\n",
    "\n",
    "s3 = S3(s3_key=\"grizly/\", file_name=\"test_scheduling.csv\").from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:35:53,480 | INFO : Submitting job s3_grizly_test_scheduling_listener...\n",
      "2020-12-01 08:35:56,230 | INFO : Job s3_grizly_test_scheduling_listener finished with status success\n",
      "2020-12-01 08:36:00,158 | INFO : Job email_upstream_success has been enqueued\n",
      "2020-12-01 08:36:01,920 | INFO : Job email_upstream_result_change has been enqueued\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-12-01 08:35:52+00:00']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit(scheduler_address=\"10.125.68.177:8999\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unregister jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-01 08:36:10,159 | INFO : Job s3_grizly_test_scheduling_listener successfully removed from registry\n",
      "2020-12-01 08:36:14,514 | INFO : Job email_upstream_success successfully removed from registry\n",
      "2020-12-01 08:36:17,853 | INFO : Job email_upstream_result_change successfully removed from registry\n"
     ]
    }
   ],
   "source": [
    "Job(\"s3_grizly_test_scheduling_listener\").unregister(remove_job_runs=True)\n",
    "Job(\"email_upstream_success\").unregister(remove_job_runs=True)\n",
    "Job(\"email_upstream_result_change\").unregister(remove_job_runs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
