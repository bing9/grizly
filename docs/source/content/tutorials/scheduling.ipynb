{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "from boto3 import resource\n",
    "\n",
    "from grizly.scheduling.registry import Job\n",
    "from grizly import Email, S3, config\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GRIZLY_REDIS_HOST\"] = \"10.125.68.177\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you register a job you have to define tasks that your job will run. Let's define a function that returns last modified date of a file in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def get_last_modified_date(full_s3_key):\n",
    "    bucket = config.get_service(\"s3\").get(\"bucket\")\n",
    "    date = resource(\"s3\").Object(bucket, full_s3_key).last_modified\n",
    "    return str(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_last_modified_date(full_s3_key=\"grizly/test_scheduling.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs that are listening for some changes are called **listener jobs**. A good practice is to start their name with `listener` prefix so that they are easy to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:16:22,026 | INFO : Job listener_s3_grizly_test_scheduling successfully removed from registry\n",
      "2020-11-18 14:16:23,008 | INFO : Job listener_s3_grizly_test_scheduling successfully registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Job(name='listener_s3_grizly_test_scheduling')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job = Job(\"listener_s3_grizly_test_scheduling\")\n",
    "\n",
    "job.register(tasks=[task], \n",
    "             if_exists=\"replace\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just registered a job called `listener_s3_grizly_test_scheduling`. The name of the job is unique and you can always check its details with `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: listener_s3_grizly_test_scheduling\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-11-18 14:16:22.028037+00:00\n",
      "crons: []\n",
      "downstream: {}\n",
      "upstream: {}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "job = Job(\"listener_s3_grizly_test_scheduling\")\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this job is not scheduled yet - it's not a cron job and it doesn't have any upstream jobs and it doesn't have any triggers. You can pass these parameters during registration or overwrite them later using `crons`, `upstream` or `triggers` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cron string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add now a cron string to our job to run every two hours. You can generate cron string using this website https://crontab.guru/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: listener_s3_grizly_test_scheduling\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-11-18 14:16:22.028037+00:00\n",
      "crons: ['0 */2 * * *']\n",
      "downstream: {}\n",
      "upstream: {}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "job.crons = \"0 */2 * * *\"\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your job imediately using `submit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mismatched versions found\n",
      "\n",
      "+-------------+--------+-----------+---------+\n",
      "| Package     | client | scheduler | workers |\n",
      "+-------------+--------+-----------+---------+\n",
      "| cloudpickle | 1.4.1  | 1.6.0     | 1.4.1   |\n",
      "+-------------+--------+-----------+---------+\n",
      "2020-11-18 14:16:32,786 | INFO : Submitting job listener_s3_grizly_test_scheduling...\n",
      "2020-11-18 14:16:37,670 | INFO : Job listener_s3_grizly_test_scheduling finished with status success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-11-18 14:15:19+00:00']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check job's last run details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first run you will be able to access `last_run` property with information about the last run of your job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\n",
      "name: None\n",
      "created_at: 2020-11-18 14:16:33.115320+00:00\n",
      "finished_at: 2020-11-18 14:16:37.671481+00:00\n",
      "duration: 3\n",
      "status: success\n",
      "error: None\n",
      "result: ['2020-11-18 14:15:19+00:00']\n"
     ]
    }
   ],
   "source": [
    "job.last_run.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now update the file and run the job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:16:41,550 | INFO : Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2020-11-18 14:16:45,338 | INFO : Successfully uploaded 'test_scheduling.csv' to S3\n"
     ]
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = DataFrame(data=d)\n",
    "\n",
    "s3 = S3(s3_key=\"grizly/\", file_name=\"test_scheduling.csv\").from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:16:46,512 | INFO : Submitting job listener_s3_grizly_test_scheduling...\n",
      "2020-11-18 14:16:51,305 | INFO : Job listener_s3_grizly_test_scheduling finished with status success\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-11-18 14:16:46+00:00']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 2\n",
      "name: None\n",
      "created_at: 2020-11-18 14:16:46.799387+00:00\n",
      "finished_at: 2020-11-18 14:16:51.306814+00:00\n",
      "duration: 3\n",
      "status: success\n",
      "error: None\n",
      "result: ['2020-11-18 14:16:46+00:00']\n"
     ]
    }
   ],
   "source": [
    "job.last_run.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register jobs with upstream job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now register two jobs with upstream job `listener_s3_grizly_test_scheduling`. One will send an email whenever upstream finished with status `success` and the other will send an email whenever the upstream changed his result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def send_email(subject, body, to):\n",
    "    logger = logging.getLogger(\"distributed.worker\").getChild(\"email\")\n",
    "    e = Email(subject=subject, body=body, logger=logger)\n",
    "    e.send(to=to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:17:02,071 | INFO : Job email_upstream_succcess successfully removed from registry\n",
      "2020-11-18 14:17:04,937 | INFO : Job email_upstream_succcess successfully registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: email_upstream_succcess\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-11-18 14:17:02.355410+00:00\n",
      "crons: []\n",
      "downstream: {}\n",
      "upstream: {'listener_s3_grizly_test_scheduling': 'success'}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "to = config.get_service(\"email\").get(\"address\")\n",
    "\n",
    "task = send_email(subject=\"Job success\",\n",
    "                   body=\"Job `listener_s3_grizly_test_scheduling` finished with status success.\", \n",
    "                   to=to)\n",
    "\n",
    "job = Job(\"email_upstream_succcess\")\n",
    "\n",
    "job.register(tasks=[task], \n",
    "             if_exists=\"replace\",\n",
    "             upstream={\"listener_s3_grizly_test_scheduling\": \"success\"}\n",
    "             )\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:17:12,310 | INFO : Job email_upstream_result_change successfully removed from registry\n",
      "2020-11-18 14:17:15,410 | INFO : Job email_upstream_result_change successfully registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: email_upstream_result_change\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-11-18 14:17:12.605310+00:00\n",
      "crons: []\n",
      "downstream: {}\n",
      "upstream: {'listener_s3_grizly_test_scheduling': 'result_change'}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "to = config.get_service(\"email\").get(\"address\")\n",
    "\n",
    "task = send_email(subject=\"File changed\",\n",
    "                   body=\"Somebody changed 'grizly/test_scheduling.csv' file!\", \n",
    "                   to=to)\n",
    "\n",
    "job = Job(\"email_upstream_result_change\")\n",
    "\n",
    "job.register(tasks=[task], \n",
    "               if_exists=\"replace\",\n",
    "               upstream={\"listener_s3_grizly_test_scheduling\": \"result_change\"}\n",
    "              )\n",
    "\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see now that `listener_s3_grizly_test_scheduling` has two downstream jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: listener_s3_grizly_test_scheduling\n",
      "owner: None\n",
      "description: None\n",
      "timeout: 3600\n",
      "created_at: 2020-11-18 14:16:22.028037+00:00\n",
      "crons: ['0 */2 * * *']\n",
      "downstream: {'email_upstream_succcess': 'success', 'email_upstream_result_change': 'result_change'}\n",
      "upstream: {}\n",
      "triggers: []\n"
     ]
    }
   ],
   "source": [
    "job = Job(\"listener_s3_grizly_test_scheduling\")\n",
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now submit the listener job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:17:23,453 | INFO : Submitting job listener_s3_grizly_test_scheduling...\n",
      "2020-11-18 14:17:28,069 | INFO : Job listener_s3_grizly_test_scheduling finished with status success\n",
      "2020-11-18 14:17:33,259 | INFO : Job email_upstream_succcess has been enqueued\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-11-18 14:16:46+00:00']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `listener_s3_grizly_test_scheduling` job finished with status success and enqueued his downstream job `email_upstream_succcess`. Let's now change the file in s3 and run our listener job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:17:37,077 | INFO : Successfully uploaded 'test_scheduling.csv' to S3\n"
     ]
    }
   ],
   "source": [
    "d = {'col1': [1, 2], 'col2': [3, 4]}\n",
    "df = DataFrame(data=d)\n",
    "\n",
    "s3 = S3(s3_key=\"grizly/\", file_name=\"test_scheduling.csv\").from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:17:38,385 | INFO : Submitting job listener_s3_grizly_test_scheduling...\n",
      "2020-11-18 14:17:43,419 | INFO : Job listener_s3_grizly_test_scheduling finished with status success\n",
      "2020-11-18 14:17:48,239 | INFO : Job email_upstream_succcess has been enqueued\n",
      "2020-11-18 14:17:50,439 | INFO : Job email_upstream_result_change has been enqueued\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-11-18 14:17:37+00:00']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unregister jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 14:18:00,619 | INFO : Job listener_s3_grizly_test_scheduling successfully removed from registry\n",
      "2020-11-18 14:18:06,036 | INFO : Job email_upstream_succcess successfully removed from registry\n",
      "2020-11-18 14:18:10,217 | INFO : Job email_upstream_result_change successfully removed from registry\n"
     ]
    }
   ],
   "source": [
    "Job(\"listener_s3_grizly_test_scheduling\").unregister(remove_job_runs=True)\n",
    "Job(\"email_upstream_succcess\").unregister(remove_job_runs=True)\n",
    "Job(\"email_upstream_result_change\").unregister(remove_job_runs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
